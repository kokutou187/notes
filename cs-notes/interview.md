#### 项目简介

- 做的在线教育的项目，简单点说就是买课程的项目。前后端分离开发的模式，后端和前端分别有一个前台和后台的开发。后端使用到的技术有 springboot + springcloud + mybatisplus + jwt + spring security + redis + maven + easyExcel + OAuth2。前端用到的技术有 vue + element-ui + axios + nodejs。其他用到的技术还有 阿里云oss + 阿里云视频点播 + 阿里云短信服务（没开通）+ 微信支付（要企业认证）和登录 + docker + git。
- 开发遇到的问题 \**********

- 项目流程，你完成了什么功能  ***************
- 数据库的描述：（主要讲两个部分）

  - 根据阿里巴巴开发手册，外键就是逻辑上的外键，建表的时候就没有加上去

  - part 1，课程相关，多对多
    - 课程分类表 subject ，id 和 parent_id，parent_id 为0就是最上层的，可以用这两个属性来获得类别的上下级关系，查询的时候用递归查询，这样代码就不会写死，不管是二级联动还是多级联动都能查出来，不用修改代码。
    - 课程表 course， is_deleted 逻辑删除，
    - 课程章节表 chapter
    - 小节表 video
    - 简介表 description
    - 讲师表 teacher，头像属性存一个 url 然后
  - part 2，权限相关，多对多，然后菜单表和角色表，角色表和用户表之间都有一个中间表，用来存储他们的关系
    - 菜单表 / 路由表
    - 角色表
    - 用户表



#### 计算机网络

- https 加密过程
  1. 客户端请求服务器获取证书公钥
  2. 客户端(SSL/TLS)解析证书（无效会弹出警告）
     - 首先验证证书是否过期以及证书的域名是否与请求的域名相同，不相同则验证不通过；
     - 获取证书的颁发机构，然后从浏览器系统中查询该机构的机构公钥；
     - 使用机构公钥解密证书上的数字签名，得到一个哈希值 H1；
     - 浏览器使用指纹算法对证书（部分）内容进行哈希运算，得到另一个哈希值 H2；
     - 比较 H1 和 H2 是否相同，不相同说明证书被篡改过。
  3. 生成随机值
  4. 用公钥加密随机值生成密钥
  5. 客户端将秘钥发送给服务器
  6. 服务端用私钥解密秘钥得到随机值
  7. 将信息和随机值混合在一起进行对称加密
  8. 将加密的内容发送给客户端
  9. 客户端用秘钥解密信息

- 拆包和粘包

  - 拆包和粘包是在socket编程中经常出现的情况，在socket 通讯过程中，如果通讯的一端一次性连续发送多条数据包，tcp 协议会将多个数据包打包成一个 tcp 报文发送出去，这就是所谓的粘包。而如果通讯的一端发送的数据包超过一次tcp报文所能传输的最大值时，就会将一个数据包拆成多个最大tcp长度的tcp报文分开传输，这就叫做拆包。

  - MTU（最大传输单元），一般用来说明TCP/IP四层协议中数据链路层的最大传输单元，普遍使用的以太网的 MTU 是1500，即最大只能传输1500字节的数据帧。

  - MSS（Maximum Segment Size，TCP建立连接后双方约定的可传输的最大TCP报文长度），如果 MTU是1500，那么 MSS = 1500 - 20(IP Header) - 20(TCP Header) = 1460byte

  - ![01](https://raw.githubusercontent.com/kokutou187/notes/master/images/01.jpg)

  - 出现粘包的原因

    1. 要发送的数据小于TCP发送缓冲区的大小，TCP将多次写入缓冲区的数据一次发送出去

    2. 接收数据端的应用层没有及时读取接收缓冲区中的数据；
    3. 数据发送过快，数据包堆积导致缓冲区积压多个数据后才一次性发送出去(如果客户端每发送一条数据就睡眠一段时间就不会发生粘包)；

  - 拆包出现的原因

    - 如果数据包太大，超过MSS的大小，就会被拆包成多个TCP报文分开传输。但是可能会因为接收端一次性的读取接收缓冲区，导致输出的格式看不出来，这时候可以通过 tcpdump -i lo0 55533来监听lo0的55533端口，来查看tcp 数据包

  - 解决方法

    - 对于粘包的情况，要对粘在一起的包进行拆包。对于拆包的情况，要对被拆开的包进行粘包。比较通用的做法就是每次发送一个应用数据包前在前面加上四个字节的包长度值，指明这个应用包的真实长度。

- post 和 put的区别

  - put 是幂等的，后来的提交会覆盖之前对同一个事物的提交请求，所以用来修改
  - post 不是幂等的，后来的请求不会覆盖之前的，所以用来新增

- HTTP 不同版本的区别

  - HTPP 1.0: 

    - 无状态，无连接的应用层协议，浏览器和服务器保持短暂的连接，浏览器每次请求都需要与服务器建立一个TCP连接，服务器处理完以后立即断开 TCP 连接（无连接），服务器不跟踪每个客户，也不记录过去的请求（无状态）。这种无状态性可以借助cookie/session机制来做身份认证和状态记录。

    - 存在的问题：

      1. 无法复用连接，每次发送请求，都需要进行一次TCP连接，而TCP的连接释放过程又是比较费事的。这种无连接的特性会使得网络的利用率变低。

      2. 队头阻塞(head of line blocking)，由于HTTP1.0规定下一个请求必须在前一个请求响应到达之前才能发送，假设前一个请求响应一直不到达，那么下一个请求就不发送，后面的请求就阻塞了。

      3. 不支持断点续传，也就是说，每次都会传送全部的页面和数据。
    
  - HTTP 1.1：

    - 长连接，HTTP1.1增加Connection字段，对于同一个host，通过设置Keep-Alive保持HTTP连接不断。避免每次客户端与服务器请求都要重复建立释放建立TCP连接。提高了网络的利用率。
    - 如果客户端想关闭HTTP连接，可以在请求头中携带Connection:false 来告知服务器关闭请求。
    - 支持断点续传，通过使用请求头中的 Range 来实现。
    - 可以使用管道传输，多个请求可以同时发送，但是服务器还是按照顺序，先回应 A 请求，完成后再回应 B 请求。要是 前面的回应特别慢，后面就会有许多请求排队等着。这称为<span style="color:red">「队头堵塞」</span>。

  - HTTP 2.0：

    - http2.0是一种安全高效的下一代http传输协议。安全是因为http2.0建立在https协议的基础上，高效是因为它是通过二进制分帧来进行数据传输。

    - 二进制分帧，HTTP2.0通过在应用层和传输层之间增加一个二进制分帧层，突破了HTTP1.1的性能限制，改进传输性能。在二进制分帧层上，http2.0会将所有传输信息分割为更小的消息和帧，并对它们采用二进制格式的编码将其封装。其中，http1.X中的首部信息header封装到Headers帧中，而request body将被封装到Data帧中。HTTP2.0实现了真正的并行传输，它能够在一个TCP上进行任意数量的HTTP请求。而这个强大的功能基于“二进制分帧”的特性。

      - 流是连接中的一个虚拟信道，可以承载双向消息传输。每个流有唯一整数标识符。为了防止两端流ID冲突，客户端发起的流具有奇数ID，服务器端发起的流具有偶数ID。
      - 消息(message)：一个完整的请求或者响应，比如请求、响应等，由一个或多个 Frame 组成。
      - 帧(frame)：类型Type, 长度Length，流标识，有效载荷
      - 流标识是描述二进制frame的格式，使得每个frame能够基于http2发送，与流标识联系的是一个流，每个流是一个逻辑联系，一个独立的双向的frame存在于客户端和服务器端之间的http2连接中。一个http2连接上可包含多个并发打开的流，这个并发流的数量能够由客户端设置。

    - 多路复用/连接共享：

      - 在http1.1中，浏览器客户端在同一时间，针对同一域名下的请求有一定数量的限制，超过限制数目的请求会被阻塞。这也是为何一些站点会有多个静态资源 CDN 域名的原因之一。 多路复用允许同时通过单一的http/2 连接发起多重的请求-响应消息。有了新的分帧机制后，http/2 不再依赖多个TCP连接去实现多流并行了。每个数据流都拆分成很多互不依赖的帧，而这些帧可以交错（乱序发送），还可以分优先级，最后再在另一端把它们重新组合起来。http 2.0 连接都是持久化的，而且客户端与服务器之间也只需要一个连接（每个域名一个连接）即可。http2连接可以承载数十或数百个流的复用，多路复用意味着来自很多流的数据包能够混合在一起通过同样连接传输。当到达终点时，再根据不同<span style="color:red">帧首部的流标识符</span>重新连接将不同的数据流进行组装。

      - http1.1过程：货轮1从A地到B地去取货物，取到货物后，从B地返回，然后货轮2在A返回并卸下货物后才开始再从A地出发取货返回，如此有序往返。

        http2.0过程：货轮1、2、3、4、5从A地无序全部出发，取货后返回，然后根据货轮号牌卸载对应货物。

    - 头部压缩

      - http1.x的头带有大量信息，而且每次都要重复发送。http/2使用encoder来减少需要传输的header大小，通讯双方各自缓存一份头部字段表，既避免了重复header的传输，又减小了需要传输的大小。对于相同的数据，不再通过每次请求和响应发送，通信期间几乎不会改变通用键-值对(用户代理、可接受的媒体类型，等等)只需发送一次。
      - 如果首部发生了变化，则只需将变化的部分加入到header帧中，改变的部分会加入到头部字段表中，首部表在 http 2.0 的连接存续期内始终存在，由客户端和服务器共同渐进地更新。
      - 需要注意的是，http 2.0关注的是首部压缩，而我们常用的gzip等是报文内容（body）的压缩，二者不仅不冲突，且能够一起达到更好的压缩效果。http/2使用的是专门为首部压缩而设计的HPACK算法。
      - http/2的HPACK算法使用一份索引表来定义常用的http Header，把常用的 http Header 存放在表里，请求的时候便只需要发送在表里的索引位置即可。

    - 服务端推送

      - 服务器可以对一个客户端请求发送多个响应，服务器向客户端推送资源无需客户端明确地请求。当服务端需要主动推送某个资源时，便会发送一个 Frame Type 为 PUSH_PROMISE 的 Frame，里面带了 PUSH 需要新建的 Stream ID。意思是告诉客户端：接下来我要用这个 ID 向你发送东西，客户端准备好接着。客户端解析 Frame 时，发现它是一个 PUSH_PROMISE 类型，便会准备接收服务端要推送的流。

    - http2.0 性能瓶颈

      - 启用http2.0后会给性能带来很大的提升，但同时也会带来新的性能瓶颈。因为现在所有的压力集中在底层一个TCP连接之上，TCP很可能就是下一个性能瓶颈，比如TCP分组的队首阻塞问题，单个TCP packet丢失导致整个连接阻塞，无法逃避，此时所有消息都会受到影响。

      - nginx服务器升级http2.0协议需要满足如下条件：

        1、nginx版本高于1.9.5；

        2、--with-http_ssl_module 跟 --with-http_v2_module

        --with-http_ssl_module模块是因为http2.0协议是一种https协议。

  - HTTP 3.0

    - 基于Google的QUIC，HTTP3 背后的主要思想是放弃 TCP，转而使用基于 UDP 的 QUIC 协议。

    - 与 HTTP2 在技术上允许未加密的通信不同，QUIC 严格要求加密后才能建立连接。

    - 为了解决传输级别的队首阻塞问题（因为HTTP2.0 是基于单个 TCP 连接的），通过 QUIC 连接传输的数据被分为一些流。流是持久性 QUIC 连接中短暂、独立的“子连接”。每个流都处理自己的错误纠正和传递保证，但使用连接全局压缩和加密属性。每个客户端发起的 HTTP 请求都在单独的流上运行，因此丢失数据包不会影响其他流/请求的数据传输。

    - QUIC提高了目前使用TCP的面向连接的网络应用的性能。它通过使用用户数据报协议（UDP）在两个端点之间创建若干个多路连接来实现这一目标，其目的是为了在网络层淘汰TCP，以满足许多应用的需求，因此该协议偶尔也会获得 “TCP/2”的昵称。QUIC与HTTP/2的多路复用连接协同工作，允许<span style="color:red">多个</span>数据流独立到达所有端点，因此不受涉及其他数据流的丢包影响。相反，HTTP/2创建在传输控制协议（TCP）上，如果任何一个TCP数据包延迟或丢失，所有<span style="color:red">多路</span>数据流都会遭受队头阻塞延迟。

      

- 路由选择协议
  - 自治系统（AS，Autonomous System，内部使用同一路由协议的系统）内部的路由选择
    - RIP，内部网关协议
      - RIP 按固定的时间间隔<span style="color:red">仅和相邻路由器交换自己的路由表</span>, 经过若干次交换之后，所有路由器最终会知道到达本自治系统中任何一个网络的最短距离和下一跳路由器地址。
    - OSPF，开放最短路径优先
      - 向本自治系统中的所有路由器发送信息，发送的信息就是与相邻路由器的链路状态，链路状态包括与哪些路由器相连以及链路的度量，度量用费用、距离、时延、带宽等来表示。只有当链路状态发生变化时，路由器才会发送信息。所有路由器都具有全网的拓扑结构图，并且是一致的。相比于 RIP，OSPF 的更新过程收敛的很快。
  - 自治系统间的路由选择
    - BGP，外部网关协议
      - AS 之间的路由选择很困难，主要是由于
        - 互联网规模很大；
        - 各个 AS 内部使用不同的路由选择协议，无法准确定义路径的度量；
        - AS 之间的路由选择必须考虑有关的策略，比如有些 AS 不愿意让其它 AS 经过。
      - BGP 只能寻找一条比较好的路由，而不是最佳路由。每个 AS 都必须配置 BGP 发言人，通过在两个相邻 BGP 发言人之间建立 TCP 连接来交换路由信息。

- TCP 是面向流的，那应用层是怎么知道 HTTP 要的数据量的呢？通过 HTTP 中的 Content-Length 首部来确定实体的大小

- HTTP 方法 GET 和 POST 之间的区别
  - GET 倾向于把数据放在 URL 中，POST 放在消息体中
  - GET 一般用于获取数据，POST 一般用于提交数据
  - GET 没有副作用，POST 有副作用，所以浏览器实现时可以把 GET 的数据做缓存，而 POST 的不做缓存。所以当你刷新 POST 提交页面时浏览器会有弹框提示你填写的数据在刷新后需要重新填写
  - URL 只支持 ASCII 编码，所以 GET 请求中需要对在 URL 中的字符进行预编码

- UDP  通过自己实现 累计确认 和 超时重传 可以确保数据的可靠性

- 为什么需要四次挥手

  - 因为 TCP 是一个全双工协议，必须单独拆除每一条信道。客户端要等待服务端把数据传输完毕，如果客户端在发送 FIN = 1 的信号时，服务端已经没有数据要传送，服务端可以把 ACK 和 FIN = 1 的两次响应合成一个

- cookie 和 session 的区别（cookie中携带session_id给服务器端用来跟踪用户）
  - cookie 和session的区别是：cookie数据保存在客户端，session数据保存在服务器端。
  - cookie 的安全性较低，别人可以通过分析存储在本地的 cookie 来伪造信息，而session保存在服务器中，安全性较高。
  - cookie 可以用来跟踪会话，也可以用来保存用户喜好或者用户名密码，而session 用来跟踪会话。当我们登录网站勾选保存用户名和密码的时候，一般保存的都是cookie，将用户名和密码的cookie保存到硬盘中，这样再次登录的时候浏览器直接将cookie发送到服务端验证
  - cookie 只能存储 ASCII 型的数据，session 可以存储任何类型的数据
  - cookie只能存放4k大小的数据，不适合存放大对象，而session在服务端则没有这个限制
  
- 如果客户端禁用了 cookie 那还能使用服务端 session 吗？
  - 可以，只要把session_id发送给服务器端就行，比如可以重写 URL，将 session_id 写到 URL 中。
  - 前端也可以做一个 http request 拦截器，将 session_id 的信息写到 headers 请求头中。

- TCP UDP 的区别
  - TCP面向连接，UDP是无连接的
  - TCP提供可靠的服务，UDP尽最大努力交付，不保证可靠交付
  - TCP传输效率相对较低，UDP传输速率高
  - TCP连接是点到点，一对一的，UDP支持一对一，一对多，多对一和多对多通信

- 为什么要有 CA

  ![01](C:\Users\pansi\Desktop\笔记收纳所\images\面试\01.jpg)



#### 操作系统

- 死锁产生的必要条件
  - 互斥：一个资源每次只能被一个进程使用
  - 占有和等待：一个进程因请求资源而阻塞时，对已获得的资源保持不放
  - 不可抢占：已经分配给一个进程的资源不能强制性地被抢占，它只能被占有它的进程显式地释放。
  - 环路等待：有两个或者两个以上的进程组成一条环路，该环路中的每个进程都在等待下一个进程所占有的资源。
- 死锁的处理方法
  - 鸵鸟策略：把头埋在沙子里，假装根本没发生。因为解决死锁问题的代价很高，因此鸵鸟策略这种不采取任务措施的方案会获得更高的性能。当发生死锁时不会对用户造成多大影响，或发生死锁的概率很低，可以采用鸵鸟策略。大多数操作系统，包括 Unix，Linux 和 Windows，处理死锁问题的办法仅仅是忽略它。
- 死锁恢复
  - 利用抢占恢复
  - 利用回滚恢复
  - 通过杀死进程恢复
- 死锁检测
  - 每种类型一个资源的死锁检测，方块表示资源，圆表示进程，资源指向进程表示该资源已经分配给该进程，进程指向资源表示进程请求获取该资源。把情况画出来，有环则表示有锁。
  - 每种类型多个资源的死锁检测
    - E 向量：资源总量
    - A 向量：资源剩余量
    - C 矩阵：每个进程所拥有的资源数量，每一行都代表一个进程拥有资源的数量
    - R 矩阵：每个进程请求的资源数量
- 死锁避免
  - 安全状态的检测与死锁的检测类似，保持程序在安全状态即可。如果一个状态不是安全的，需要拒绝进入这个状态。
- 内存分配的方式：
  - 从静态存储区域分配，内存在程序编译的时候就已经分配好，这块内存在程序的整个运行期间都存在。例如全局变量。
  - 在栈上创建。在执行函数时，函数内部局部变量的存储单元都可以在栈上创建，函数执行结束时，这些存储单元自动被释放。
  - 从堆上分配，也就是动态内存分配。程序在运行时用 new 或 malloc 申请任意多少的内存。释放时机需要自己定义，但是 Java 有垃圾回收器所以比较方便。
- 进程通信的方式
  - 管道
  - FIFO
  - 共享内存
  - 套接字
  - 信号量
  - 消息队列
- 线程通信的方式
  - 锁机制：互斥锁，条件变量，读写锁
  - 信号量机制
  - 信号机制



#### 数据结构

- 哈夫曼编码的建树过程

  > 哈曼曼编码是用于压缩报文的方法，所以字母出现的权重(频率)大的，要更靠近根节点。
  > 需要传送的报文为"ABCACCDAEAE" ，A B C D E 出现的频率分别为 0.36 0.1 0.27 0.1 0.18
  >
  > 1. 按权重排序 0.1 0.1 0.18 0.27 0.36，取最小的两个权重 0.1 0.1 对应的字母为 B D
  >
  >    ```mermaid
  >    graph TD
  >    A[0.2] --> B
  >    A --> D
  >    ```
  >
  > 2. B D 相加后的权重为 0.2，排序权重 0.18 0.2 0.27 0.36，取最小的两个
  >
  >    ```mermaid
  >    graph TD
  >    A[0.38] --> E
  >    A --> J[0.2]
  >    J --> B
  >    J --> D
  >    ```
  >
  > 3. 权重排序 0.27 0.36 0.38，取最小的两个，对应 C A
  >
  >    ```mermaid
  >    graph TD
  >    K[0.38] --> E
  >    K --> J[0.2]
  >    J --> B
  >    J --> D
  >    L[0.63] --> C
  >    L --> A
  >    ```
  >
  > 4. 权重排序 0.38 0.63，权重小的在树的左边，权重大的在右边，左边为0，右边为1
  >
  >    ```mermaid
  >    graph TD
  >    K[0.38] -->|0| E
  >    K -->|1| J[0.2]
  >    J -->|0| B
  >    J -->|1| D
  >    L[0.63] -->|0| C
  >    L -->|1| A
  >    S -->|0| K
  >    S -->|1| L
  >    ```
  >
  > 5. 所以A B C D E对应的编码分别为 11  010  10  011  00。注意到B D权重一样，所以在权重一样时，哈夫曼编码可能不唯一。

- 如何解决 hash 冲突
  - 链表法
    - 数组加链表
  - 开放寻址法
    - 冲突了就顺延



#### 算法

- 快速排序

  ```java
  public void quickSort(int[] arr, int start, int end) {
      int l = start, r = end;
      if (l >= r) return;
      //基准值
      int val = arr[l];
      while (l < r) {
          while (l < r && arr[r] >= val) r--;
          // 注意判断的条件
          if (l < r) {
              //不是交换，是覆盖，基准值已经保存了
              arr[l] = arr[r];
              l++;
          }
          while (l < r && arr[l] <= val) l++;
          if (l < r) {
              arr[r] = arr[l];
              r--;
          }
      }
      //基准值该在的位置
      arr[l] = val;
      //基准值两侧的区间进行排序，双闭区间，所以一个加一，一个减一
      quickSort(arr, start, l-1);
      quickSort(arr, l+1, end);
  }
  ```

  

- 0-9十个数字构成一个环，从0开始，每次只能走一步，如果每次走N步，问有多少种回到原点的方法

  ```c++
  #include <iostream>
  /*N 个数围城一圈，每次走一步，一共走K步，走回原点，有几种走法*/
  
  using namespace std;
  
  int N=10;
  
  int f(int start, int dest, int n){
      if(n==0 && start == dest)
          return 1;
      if(n==0 && start != dest)
          return 0;
      if(n < start-dest || n < dest-start)  // 剩余的步数小于需要走的路程差
          return 0;
      return f((start + 1)%N, dest, n-1) + f((start - 1 + N)%N, dest, n-1);
  
  }
  
  
  int main()
  {
      int n;
      while(true){
          cin >> n;
          if(n<=0) return 0;
          cout << f(0, 0, n) << endl;
      }
      return 0;
  }
  ```



#### 数据库

- 联合索引

  - 最左匹配（出现的原因是 b+ 树按照最左边的数来排序）

    - 如果SQL语句中用到了联合索引的最左边的索引，那么这条SQL语句就可以利用这个联合索引去进行匹配，但是遇到范围查询（<, >, between, like) 就会停止匹配。

      > 如果对字段(a, b)建立一个联合索引，当where后的条件为
      > a = 1
      > a = 1 and b = 2
      > 是可以匹配索引的，执行
      > b = 2 and a = 1
      > 也是可以匹配索引的，他表面上看起来不符合最左匹配，但是Mysql有优化器会自动调整 a, b 的顺序与索引顺序一致。执行
      > b = 2
      > 就匹配不到索引了
      > 如果你对 (a, b, c, d)建立索引，where 后条件为
      > a = 1 and b = 2 and c > 3 and d = 4
      > 那么，a, b, c 三个字段可以用到索引，而 d 就匹配不到，因为遇到了范围查询

  - 例题

    > SELECT * FROM table_name WHERE a = 1 and b = 2 and c = 3;
    > 如何建立索引？
    > (a, b, c) , (c, b, a), (b, a, c) 都可以，重点是要将区分度高的字段放在前面，区分度低放在后面。

    > SELECT * FROM table_name WHERE a > 1 and b = 2;
    > 如何建立索引？
    > 对 (b, a) 建立索引，因为最左匹配原则遇到范围查询立即停止，对(b, a)建立索引那么两个字段都能用上，优化器会帮我们调整where后 a, b 的顺序，让我们用上索引。

    > SELECT * FROM table_name WHERE a > 1 and b = 2 and c > 3;
    > (b, a) 或者 (b, c) 都可以

    > SELECT * FROM table_name WHERE a > 1 ORDER BY b;
    > 对 (a) 建立索引，因为 a 值是一个范围，这个范围内 b 值是无序的，没有必要对 (a,b)  建立索引

    > SELECT * FROM table_name WHERE a IN (1, 2, 3) and b > 1;
    > 对(a, b)建立索引，因为 IN 在这里可以视为等值引用，不会终止索引匹配

    > SELECT * FROM table_name WHERE a = 1 AND b IN (1, 2, 3) AND c > 3 ORDER BY c;
    > (a, b) ，因为此时 c 排序是用不到索引的

- MySQL 索引（大多数为B+树索引）
  - 主索引的叶子节点的 data 域记录着完整的数据记录，这种索引方式被称为聚簇索引，因为无法把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。
  - 辅助索引的叶子节点的 data 域记录着主键的值，因此在使用辅助索引进行查找时，需要先查找到主键值，然后再到主索引中进行查找。

- B+ 树
  - 进行查找操作时，首先在根节点进行二分查找，找到一个 key 所在的指针，然后递归地在指针所指向的节点进行查找。直到查找到叶子节点，然后在叶子节点上进行二分查找，找出 key 所对应的 data。
  - 与红黑树比较
    - B+ 树有更低的树高。平衡树的树高 O(h)=O(logdN)，其中 d 为每个节点的出度。红黑树的出度为 2，而 B+ Tree 的出度一般都非常大，所以红黑树的树高 h 很明显比 B+ Tree 大非常多。
    - 磁盘访问原理。操作系统一般将内存和磁盘分割成固定大小的块，每一块称为一页，内存与磁盘以页为单位交换数据。数据库系统将索引的一个节点的大小设置为页的大小，使得一次 I/O 就能完全载入一个节点。如果数据不在同一个磁盘块上，那么通常需要移动制动手臂进行寻道，而制动手臂因为其物理结构导致了移动效率低下，从而增加磁盘数据读取时间。B+ 树相对于红黑树有更低的树高，进行寻道的次数与树高成正比，在同一个磁盘块上进行访问只需要很短的磁盘旋转时间，所以 B+ 树更适合磁盘数据的读取。
  - 与 B 树比较
      - 叶子节点连接在一起，对范围查找更友好
  
- 聚簇索引和非聚簇索引
  - 聚簇索引将数据存储与索引放在一块，找到索引也就找到了数据。非聚簇索引中辅助索引的叶子节点存放的是对应的主键，所以使用辅助索引时需要再对主键索引一次。
  - 非聚簇索引表数据和索引存储在不同的地方，辅助索引和主键索引的叶子节点都使用一个地址指针指向了真正的表数据，所以对于表数据来说辅助索引和主键索引没有任何差别。索引树都是独立的，通过辅助索引检索无需访问主键的索引树。不用二次查询



#### 实际问题

- 10G 的文件，1G内存如何排序
  - 分别排序：
    - 根据内存1G，数据10G，我们将10G数据切分成10份，通过内存调用磁盘的方式，每1G进行排序，排序结束后，我们会得到10个有序的数据数组。
  - 归并排序
    - 多路归并过程可以使用败者树或最小堆。为方便起见我还是用最小堆吧，原理是一样的。
    - 内存中开辟一个大小为10的最小堆，和一个缓冲区（小于1G，不要太小）。
       取10份排序好的数据的首位进入最小堆。则最小的数位于堆顶，移除堆顶元素并写入缓冲区，然后将移除元素的元素所属数组中的下一位进入最小堆，再次移除堆顶进入缓冲区...直到缓冲区满，缓冲区回写磁盘，清空缓冲区，再次将数据置入最小堆。



#### Java基础

- 面向对象的三大特点：封装、继承、多态

- synchronize 和 volatile 自己码实现
  - monitorenter 和 monitorexit
  - ACC_Volatile

- 多线程怎么设置线程数
  - CPU密集型：操作内存处理的业务，一般线程数设置为：CPU核数 + 1 或者 CPU核数 * 2.核数为4一般设置为5到8
  - IO密集型：文件操作，网络操作，数据库操作，一般线程设置为：cpu核数 / (1-0.9)，核数为4一般设置为40

- 线程池为什么需要设置缓冲队列（我的理解）：
  1. 怕突然出现峰值流量，直接把线程池打垮
  2. 根据具体的业务场景来看吧：如果你想要响应速度快，比如电商的一些详情页面，如果一段时间没有加载出来那么用户很有可能直接流失，所以我们应该少设置缓冲队列，而扩大最大线程数，并且尽可能让核心线程数等于最大线程数。但是如果你的任务没有要求要在很短的时间内完成，那么我们可以利用更少的资源做一样的事，把剩余的资源让出来
  3. 当核心线程数满了且等待队列满了，但是未达到最大线程数时，需要新建线程，如果之后这个线程空闲了，还需要回收这个线程，那么线程的创建和回收的资源消耗也是挺大的
- HashMap resize可能导致的问题：resize死循环。HashMap默认的负载因子是0.75，默认容量是16。数组 + 链表 + 红黑树来实现，当一个冲突达到 8 时，链表自动转化为红黑树。
- ConcurrentHashMap线程安全原理：
  - 分段机制（segment），每段加 reentrantLock 可重入锁。
  - 定位元素，（1）找segment数组下标 （2）找 segment 的 HashEntry 数组下标。
  - get 方法：不需要加锁，value使用了 volatile 关键字修饰
  - put 方法：hash计算段-->锁定段 --> hash计算 HashEntry 数组 --> 若超过阈值 --> 扩容 --> 释放。put 过程中会 modCount + 1，为了后续的计算大小。
  - size() 方法：为了求Map的大小，需要全局锁，但是性能差。于是采用modCount计算器，用于记录段的大小。
  - clear()时不加锁，所以多线程中可能出现数据不一致。
- HashTable，HashMap，ConcurrentHashMap的关系/区别
  - HashMap 内部是使用 Entry数组存储数组结构，HashTable相当于在整个 Entry 数组上加上了一把大锁，ConcurrentHashMap则是将 Entry 数组进行分段，分成若干个 Segment，对 Segment 加锁。
- 为什么 NIO(非阻塞IO) 比 IO(阻塞IO) 更快：
  - I/O 与 NIO 最重要的区别是数据打包和传输的方式，I/O 以流的方式处理数据，而 NIO 以块的方式处理数据。也就是 IO 一次处理一个字节的数据，而 NIO 一次可以处理一块的数据
- 为什么 NIO 在网络中广泛应用 / 有什么好处
  - NIO 实现了 IO 多路复用中的<span style="color:red"> Reactor 模型</span>，一个线程 Thread 使用一个选择器 Selector 通过轮询的方式去监听多个通道 Channel 上的事件，从而让一个线程就可以处理多个事件。
  - 通过配置监听的通道 Channel 为非阻塞，那么当 Channel 上的 IO 事件还未到达时，Selector 就不会进入阻塞状态一直等待，而是继续轮询其它 Channel，找到 IO 事件已经到达的 Channel 执行。
  - 因为创建和切换线程的开销很大，因此使用一个线程来处理多个事件而不是一个线程处理一个事件，对于 IO 密集型的应用具有很好地性能。
  - 应该注意的是，只有套接字 Channel 才能配置为非阻塞，而 FileChannel 不能，为 FileChannel 配置非阻塞也没有意义。因为 FileChannel 没有切换到非阻塞状态的方法（集成体系不同，没有 configureBlocking 方法）。
- ThreadLocal 有什么作用
  - ThreadLocal 是一个关于创建线程局部变量的类，通常情况下，我们创建的变量是可以被任何一个线程访问并修改的。而使用ThreadLocal创建的变量只能被当前线程访问，其他线程则无法访问和修改。



#### Redis

- redis 是 IO 多路复用的，实现了<span style="color:red"> Reactor 模型</span>。
- redis 主从复制的过程：
  - 主服务器创建文件，发送给从服务器，并在发送期间使用缓冲区记录执行的写命令。快照文件发送完毕之后，开始向从服务器发送存储在缓冲区中的写命令；
  - 从服务器丢弃所有旧数据，载入主服务器发来的快照文件，之后从服务器开始接受主服务器发来的写命令；
  - 主服务器每执行一次写命令，就向从服务器发送相同的写命令。
- redis 选主过程：
  - slave发现自己的master变为FAIL
  - 将自己记录的集群currentEpoch加1，并广播Failover Request信息
  - 其他节点收到该信息，只有master响应，判断请求者的合法性，并发送FAILOVER_AUTH_ACK，对每一个epoch只发送一次ack
  - 尝试failover的slave收集FAILOVER_AUTH_ACK，超过半数后变成新Master，广播Pong通知其他集群节点



#### 分布式

- 分布式唯一ID生成方案

  - 字符串ID

    ```java
    String uuid = UUID.randomUUID().toString().replaceAll("-", "");
    ```

    > 优点：性能非常好，本地生成，没有网络消耗
    > 缺点：UUID生成的标识写入数据库的性能差，因为生成的 36 个字符是无序的，不能生成递增有序的数字。写入的目标页可能不在数据库的缓存区，那么就需要大量的换入换出，导致性能低下。也有地方说插入不规则会可能会导致频繁的分裂/分页，而B树的叶子节点的分裂会导致大量的读写操作，磁头需要频繁移动，而且数据还有不规整，导致数据碎片。但是即使顺序插入，也有可能会导致叶子节点的分裂，因为叶子节点的数量要维持在 [L/2] ~ L之间，可能相比不规则还是少一点。而且B+树的叶子节点用链表维护的话，递增就不用进行复杂的重新排序了，还是快的。

  - 数据库自增主键（单机、集群分布式两种）

    > 单机：
    >
    > 使用数据库的 auto_increment 属性，每次设置 ID 的值就使用 LAST_INSERT_ID来设置，如下：
    > UPDATE  ticket SET value=LAST_INSERT_ID()  WHERE name='USER'LAST_INSERT_ID即为最后插入的ID值，根据MySQL的官方手册说明，它有2种使用方法：
    >
    > 1. 是不带参数：LAST_INSERT_ID()，这种方法和AUTO_INCREMENT属性一起使用，当往带有‘AUTO_INCREMENT’属性字段的表中新增记录时，LAST_INSERT_ID()即返回该字段的值；
    >
    > 2. 是带有表达式：如LAST_INSERT_ID(value+1)，它返回的是表达式的值，即‘value+1’；
    >
    > 优点：实现简单、便捷，数据库保障唯一性、保障 id 递增
    > 缺点：可用性难以保证，数据库水平扩展比较难
    >
    > 集群分布式（不好用）：
    >
    > 可以根据机器的数量定义步长，比如 a, b, c 三台机器，第一次获取 id, 从 a, b, c分别取1, 2, 3，下一次取 4, 5, 6。但是如果这时候要扩容一台，我们可以把 b, c的初始值设置比 a 超过很多,　然后加一台新的，但是如果是100台甚至更多，那就很复杂了。但是这种方案数据库压力很大，每次获取ID都得读写一次数据库，非常影响性能，不符合分布式ID里的<span style="color:red">延迟低</span>和<span style="color:red">高QPS</span>的规则。

  - 基于Redis生成全局ID策略

    > 因为Redis是单线程的天生保证原子性，可以使用原子操作，而且是存在内存中读写速度快于MySQL
    >
    > 集群分布式：
    > 使用Redis集群获得更高的吞吐量，假如一个集群中有5个Redis，那么步长就为5来生成。缺点是如果一台Redis挂了，就会出现序号不连续

  - Twitter的分布式自增 ID 算法 snowflake(雪花)

    > twitter的SnowFlake生成的ID能够按照时间有序生成，id为一个64bit大小的整数，为一个Long型。分布式系统内不会产生ID碰撞，并且效率较高。结构为：1bit不用，41bit时间戳，10bit工作机器id，12bit序列号。导入hutool-captcha工具包，使用IdUtil.createSnowflake(long, long);
    > 优点：毫秒数在高位，自增序列在地位，整个ID都是趋势递增的，不依赖数据库等第三方系统，以服务的方式部署，稳定性更高
    >
    > 缺点：依赖时钟机制，如果机器时钟回拨，会导致ID重复生成。分布式环境，每台机器时钟不可能完全同步，有时会出现全局递增的情况（但是无伤大雅，一般分布式ID只要求趋势递增，并不要求严格递增，即Ａ服务拿到的ID可能一直比同一时刻或者误差时间内从B服务拿到的ID值大，因为毫秒数在高位）。
  
- 分布式限流

  - 限流可以应对：热点业务带来的突发请求，调用方bug导致的突发请求，恶意攻击请求。所以对公开的接口最好采取限流措施。

    - 当应用为单点应用时，只要应用就行了限流，那么应用所依赖的各种服务也都得到了保护。
    - 对于分布式系统，单节点的限流仅能保护自身节点，但无法保护应用依赖的各种服务，并且在进行节点扩容、缩容时也无法准确控制整个服务的请求限制。如果实现了分布式限流，那么就可以方便地控制整个服务集群的请求限制，且保护了服务依赖的各种资源。

  - 分布式限流算法

    - 固定窗口计数器

      > 将时间分为多个窗口，在每个窗口内每有一次请求就将计数器加一，如果计数器超过了限制数量，则本窗口内所有的超过限制数的请求都被丢弃。当时间到达下一个窗口时，计数器重置。这个算法有时会让通过的请求量为限制数的两倍。比如说：限制数为5，0-1秒中的0.5-1s 通过了5个请求，1-2s 中的 1-1.5s 通过了5个请求，那么0-1s, 1-2s 都只通过5个请求，但是在 0.5-1.5s 这一秒内通过的请求数为 10。

    - 滑动窗口计数器

      > 将时间划分成多个区间，在每个区间内每有一次请求就将计数器加一，维持一个时间窗口，占据多个区间，每经过一个区间的时间，则抛弃最老的一个区间，并纳入最新的一个区间。超出窗口的限制数的请求将会被抛弃

    - 漏桶算法

      > 来的请求代表进入桶的水滴，桶的容量表示允许存储的最大请求数，但是桶是漏的，会漏水，漏水的速率也就是处理/完成请求的速率。如果桶满了，多余的请求将被直接丢弃。多用队列来实现，缺点是当短时间内有大量的突发请求时，即便此时服务器没有任何负载，每个请求也得在队列中等待一段时间才能被响应。

    - 令牌桶算法（常用）

      > 桶装的是令牌，令牌以固定速率生成，超出桶限制数的令牌被丢弃，每个请求都需要从桶中取一个令牌才可以被解决。如果桶空了，那么取不到令牌的请求会被丢弃。

  - 代码实现

    - Google 的开源项目 guava(番石榴) 提供了 RateLimiter类，实现了单点的令牌桶限流
    - 分布式限流框架有：Hystrix, resilience4j, Sentinel 等框架
    - 脚本编写，redis + lua脚本实现令牌桶算法，对分布式系统进行统一的全局限流



#### Spring

- SpringCloud 的组件
  - eruka 和 nacos 做注册中心
  - Ribbon 负载均衡
  - Hystrix 做熔断器
  - Spring Cloud GateWay 做网关
  - Feign 做远程调用
  - Spring Cloud Stream做消息驱动
- Spring 和 SpringBoot 比较
  - Spring 框架为开发 Java 应用程序提供了全面的基础架构支持。它包含了一些很好的功能，如：依赖注入和开箱即用的模块。
  - SpringBoot 基本上是 Spring 框架的扩展，它消除了 Spring 应用程序所需的 XML配置，可以更快更高效的进行开发。
- SpringBoot 的一些特征：
  - 嵌入式的 Tomcat，Jetty 容器
  - 尽可能自动化配置 spring 应用
  - 提供了便捷的配置方式。
  - 对于maven中的依赖只需要引入对应的 starter 就行



#### 前端

- ajax 的原理
  - ajax 借用 XmlHttpRequest 对象来进行异步访问。异步传输是面向字符的传输，单位是字符（通常是8比特位的字符），而同步传输是面向比特的传输，单位是帧。
  - 具体来说，异步传输是将比特分成小组来进行传送。一般每个小组是一个8位字符，在每个小组的头部和尾部都有一个开始位和一个停止位。所以传输的有效位只有 6 / 8  = 75%，虽然有效率比不上同步传输，但是就像是停水的时候（其他数据传输），异步传输你还能得到水，只是水流变小了，而此时同步传输则完全停水了。